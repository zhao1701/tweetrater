{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 03 - Classification with Naive Bayes\n",
    "In this section, we will assess the performance of various Naive Bayes models. In particular, we will examine, in order of complexity:\n",
    "- Bernoulli Naive Bayes with term presence vectors\n",
    "- Multinomial Naive Bayes with term frequency vectors\n",
    "- Multinomial Naive Bayes with [td-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (term frequency, inverse document frequency) vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "[Naive Bayes classifiers](http://sebastianraschka.com/Articles/2014_naive_bayes_1.html) are based off Bayes' theorem of conditional probability, and though they are simple, have performed well particularly for text classification. They are labelled naive because the model assumes conditional independence of features, that is, the presence of a word in a tweet does not affect the probability of other words being observed in the same tweet. Though this assumption does not hold, in practice, the violation of conditional independence does not significantly undermine the accuracy of Naive Bayes in text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle as pkl\n",
    "\n",
    "pd.options.display.max_colwidth = 400\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load data\n",
    "Because Naive Bayes models classify tweets from their [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) representation, these models do not account for the sequence or semantic meaning of words. Thus, the difference between \"swam\" and \"swimming\" is inconsequential to the model, and both words should be represented with the same root of \"swim\" to prevent an explosion in tweet vector dimensionality. Controlling dimensionality is of special concern given the limited number of tweets in the corpus, and for this reason, stop words, which contribute little to no useful information about the offensiveness of a tweet, should also be ignored. So for classification with Naive Bayes models, we will use lemmatized tweets generated from **02 - Data Wrangling**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>rating</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>853718217</th>\n",
       "      <td>warning : penny board will make -PRON- a faggot</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853718218</th>\n",
       "      <td>fuck dyke</td>\n",
       "      <td>2</td>\n",
       "      <td>0.7227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853718219</th>\n",
       "      <td>twitter_handle twitter_handle twitter_handle twitter_handle twitter_handle at least i do not look like jefree starr faggot</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                 text  \\\n",
       "id                                                                                                                                      \n",
       "853718217                                                                             warning : penny board will make -PRON- a faggot   \n",
       "853718218                                                                                                                   fuck dyke   \n",
       "853718219  twitter_handle twitter_handle twitter_handle twitter_handle twitter_handle at least i do not look like jefree starr faggot   \n",
       "\n",
       "           rating  confidence  \n",
       "id                             \n",
       "853718217       1      0.6013  \n",
       "853718218       2      0.7227  \n",
       "853718219       2      0.5229  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = os.path.join('..','data','dataframe_clean')\n",
    "with open(data_path, 'rb') as file_in:\n",
    "    df_clean = pkl.load(file_in)\n",
    "    \n",
    "df_clean.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Bernoulli Naive Bayes with term presence vectors\n",
    "Given a corpus with a vocabulary of size $v$, each tweet can be represented by a $v$-dimensional vector where each index is associated with a unique term in the vocabulary, a value of 1 indicates the presence of that term in the tweet, and 0 indicates otherwise. Because each tweet is converted to a vector of binary features, a Bernoulli Naive Bayes model can be applied to learn the offensiveness of each tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create train and test set\n",
    "Of the roughly 13,000 tweets, we hold out 30% for the test set and ensure the proportion of non-offensive, offensive, and hateful tweets are the same for both training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Extract cleaned text of each tweet and split into training and test sets\n",
    "X = df_clean.text\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_clean.text, df_clean.rating,\n",
    "                                                    test_size = 0.3, stratify = df_clean.rating, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Construct and fit pipeline\n",
    "We build a pipeline consisting of a count vectorizer that converts tweets into binary vectors and the Bernoulli Naive Bayes classifier itself. To optimize the model, we grid search through combinations of hyperparameters:\n",
    "- **min_df**: A term with a document frequency (the proportion of documents that contain said term) below this threshold is omitted from the vocabulary. This is useful for removing terms that are so rare that they can cause a model to overfit on their infrequent presence.\n",
    "- **max_df**: A term with a document frequency above this threshold is omitted from the vocabulary. This serves a similar function to removing stop words, but adapts to the contents of a corpus.\n",
    "- **alpha**: A value for calculating a fail-safe probability in the event of observing an unknown term in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:\n",
      " {'vect__max_df': 0.11500000000000002, 'bnb__alpha': 0.25, 'vect__min_df': 0.0040000000000000001}\n",
      "Best accuracy:  0.7705132396207911\n"
     ]
    }
   ],
   "source": [
    "steps = [('vect', CountVectorizer(binary=True)), ('bnb', BernoulliNB())]\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "parameters = {'vect__min_df': np.arange(0,0.010, 0.002),\n",
    "              'vect__max_df': np.arange(.10,.142, 0.005),\n",
    "              'bnb__alpha': np.arange(0.0,0.4,0.05)}\n",
    "\n",
    "pipe_cv_bnb = GridSearchCV(pipe, param_grid = parameters, cv = 3, scoring = 'accuracy')\n",
    "pipe_cv_bnb.fit(X_train, y_train)\n",
    "print('Best parameters:\\n', pipe_cv_bnb.best_params_)\n",
    "print('Best accuracy: ', pipe_cv_bnb.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Optimized hyperparameters produce a Bernoulli Naive Bayes model with a cross-validated accuracy of 0.775. Of particular interest is the surprisingly low max_df hyperparameter, which suggests just removing stop words fell far short of pruning the vocabulary of extraneous terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6EAAAGICAYAAACwZ+b9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJElSbf4/QWFyFYIU0yAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110bd6f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cv_scores = [score[1] for score in pipe_cv_bnb.grid_scores_ if score[0]['bnb__alpha'] == 0.30000000000000004]\n",
    "min_df = pipe_cv_bnb.param_grid['vect__min_df']\n",
    "max_df = pipe_cv_bnb.param_grid['vect__max_df']\n",
    "cv_scores = np.array(cv_scores).reshape(len(max_df), len(min_df))\n",
    "cv_scores\n",
    "\n",
    "plt.figure(figsize = (10, 4))\n",
    "g = plt.contourf(min_df, max_df, cv_scores, cmap = 'plasma', levels = np.linspace(0.7, 0.79, 20))\n",
    "plt.colorbar(g)\n",
    "plt.title('Cross-validated accuracy by document frequency hyperparameters')\n",
    "plt.xlabel('min_df')\n",
    "plt.ylabel('max_df')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In fact, the vocabulary used by the model consists of only 261 terms. We can even feed each term back into the model as a one-word tweet and have the model output a probability that the tweet is *not* offensive, allowing us to identify the most and least offensive terms. The list of most offensive terms is largely unsurprising, but the inclusion of \"outta\" is interesting; though not offensive individually, it appears so frequently in offensive tweets that the model considers its presence a strong indicator of offensive speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 361 \n",
      "\n",
      "MOST OFFENSIVE TERMS\n",
      "term      bitch     nigga    niggas    nigger   faggot      fuck    fuckin  \\\n",
      "proba  0.000852  0.001044  0.002586  0.003927  0.00427  0.010885  0.013833   \n",
      "\n",
      "term      queer   fucking       hoe       ass    cuffin      cunt     pussy  \\\n",
      "proba  0.018027  0.020904  0.023783  0.024516  0.035443  0.035614  0.046433   \n",
      "\n",
      "term        gay      shit      dumb      shut   bastard      fag  \n",
      "proba  0.070205  0.088139  0.103561  0.106553  0.112064  0.11435  \n",
      "\n",
      "LEAST OFFENSIVE TERMS\n",
      "term   religion       bag     cover     news   control     under       via  \\\n",
      "proba  0.881537  0.882742  0.883438  0.88656  0.888434  0.894393  0.895111   \n",
      "\n",
      "term   tomorrow      rule      book     blame       pal  married       tie  \\\n",
      "proba  0.909266  0.911248  0.926921  0.926941  0.932365   0.9428  0.954271   \n",
      "\n",
      "term    wallet     equal  ultimate  opportunity   version   usually  \n",
      "proba  0.96005  0.961089  0.961892      0.96551  0.968077  0.970361  \n"
     ]
    }
   ],
   "source": [
    "vect = pipe_cv_bnb.best_estimator_.steps[0][1]\n",
    "vocab_size = len(vect.vocabulary_)\n",
    "print('Vocabulary size:', vocab_size,'\\n')\n",
    "identity = np.eye(vocab_size)\n",
    "\n",
    "estimator = pipe_cv_bnb.best_estimator_.steps[1][1]\n",
    "probs = estimator.predict_proba(identity)[:,0]\n",
    "\n",
    "words = sorted(list(vect.vocabulary_.keys()))\n",
    "words_df = pd.DataFrame(dict(term = words, proba = probs))\n",
    "words_df = words_df[['term', 'proba']].set_index('term')\n",
    "\n",
    "print('MOST OFFENSIVE TERMS')\n",
    "print(words_df.sort_values(by = 'proba').head(20).T)\n",
    "print()\n",
    "print('LEAST OFFENSIVE TERMS')\n",
    "print(words_df.sort_values(by = 'proba').tail(20).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally, we apply the model on holdout data to test its generalizability to the remainder of the tweets. In addition to accuracy, we are interested in its weighted F1-score, a weighted average of F1-scores for each label that serves as a compromise between [micro-averaging and macro-averaging](http://www.cnts.ua.ac.be/~vincent/pdf/microaverage.pdf) a multi-class F1-score. We also calculate normalized confusion matrices so that the matrices for the training data and test data are comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def evaluate(y, y_pred):\n",
    "    print('Accuracy:', accuracy_score(y, y_pred))\n",
    "    print('F1 weighted: ', f1_score(y, y_pred, average = 'weighted'))\n",
    "    \n",
    "    matrix = confusion_matrix(y, y_pred)\n",
    "    matrix = matrix / matrix.sum().sum()\n",
    "    print('Normalized confusion matrix:\\n', matrix)\n",
    "    \n",
    "    y_values = y.value_counts()\n",
    "    y_values = y_values / y_values.sum()\n",
    "    print('Actual proportions:\\n', np.array(y_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_train_pred_bnb = pipe_cv_bnb.predict(X_train)\n",
    "y_test_pred_bnb = pipe_cv_bnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As expected, the performance of the model on the test data is slightly worse than that of the training data. Where the model tends to struggle significantly is in distinguishing between offensive and hate speech. In fact, the majority of tweets labelled hate speech are mis-classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.791217173368\n",
      "F1 weighted:  0.784842359671\n",
      "Normalized confusion matrix:\n",
      " [[ 0.488068    0.02299226  0.0054484 ]\n",
      " [ 0.04565762  0.2327558   0.0518688 ]\n",
      " [ 0.01994116  0.06287458  0.07039337]]\n",
      "Actual proportions:\n",
      " [ 0.51650866  0.33028223  0.15320911]\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_train, y_train_pred_bnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.775800711744\n",
      "F1 weighted:  0.767784315586\n",
      "Normalized confusion matrix:\n",
      " [[ 0.48652771  0.02592781  0.00406711]\n",
      " [ 0.04600915  0.22877478  0.05566853]\n",
      " [ 0.02008134  0.07244535  0.06049822]]\n",
      "Actual proportions:\n",
      " [ 0.51652262  0.33045247  0.15302491]\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test, y_test_pred_bnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Two-class performance evaluation\n",
    "That the line between offensive and hateful speech is fuzzy even for humans begs the question of how the model would perform if the distinction is removed and hateful speech is considered offensive. In this case, the model's accuracy on the test data would have been an impressive 90.7%, with most of the mis-classifications occuring as a result of classifying offensive speech as non-offensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.903914590747\n",
      "F1 weighted:  0.903673640662\n",
      "Normalized confusion matrix:\n",
      " [[ 0.48652771  0.02999492]\n",
      " [ 0.06609049  0.41738688]]\n",
      "Actual proportions:\n",
      " [ 0.51652262  0.48347738]\n"
     ]
    }
   ],
   "source": [
    "y_binary = (y_test == 1) | (y_test == 2)\n",
    "y_pred_binary_bnb = (y_test_pred_bnb == 1) | (y_test_pred_bnb == 2)\n",
    "evaluate(y_binary, y_pred_binary_bnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../data/model_bnb', 'wb') as file_out:\n",
    "    pkl.dump(pipe_cv_bnb, file_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Multinomial Naive Bayes with term frequency vectors\n",
    "In a Bernoulli Naive Bayes model, because each vector element only indicates the presence of a particular term, the model does not account for the multiplicity of terms within a tweet. That is, whether a term occurs just once within a tweet or multiple times makes no difference to the model. By changing the vector representation to a term frequency vector and, by necessity, using a Multinomial Naive Bayes classifier, we can incorporate additional information about each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:\n",
      " {'vect__max_df': 0.11000000000000001, 'mnb__alpha': 0.0, 'vect__min_df': 0.0060000000000000001}\n",
      "Best accuracy:  0.7764192139737991\n"
     ]
    }
   ],
   "source": [
    "steps = [('vect', CountVectorizer()),('mnb', MultinomialNB())]\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "parameters = {'vect__min_df': np.arange(0,0.010, 0.002),\n",
    "              'vect__max_df': np.arange(.10,.145, 0.002),\n",
    "              'mnb__alpha': np.arange(0.0,0.5,0.1)}\n",
    "\n",
    "pipe_cv_mnb = GridSearchCV(pipe, param_grid = parameters, cv = 3, scoring = 'accuracy')\n",
    "pipe_cv_mnb.fit(X_train, y_train)\n",
    "\n",
    "print('Best parameters:\\n', pipe_cv_mnb.best_params_)\n",
    "print('Best accuracy: ', pipe_cv_mnb.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The grid search for optimized hyperparameters for Multinomial Naive Bayes returns values for mininum and maximum document frequency that further lower the vocabulary size to 161 terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 161 \n",
      "\n",
      "MOST OFFENSIVE TERMS\n",
      "term   niggas  bitch  nigga    faggot    nigger    fuckin      fuck      hoe  \\\n",
      "proba     0.0    0.0    0.0  0.004143  0.004331  0.010586  0.013714  0.01569   \n",
      "\n",
      "term        ass    cuffin      cunt   fucking     queer       gay      shit  \\\n",
      "proba  0.021801  0.024025  0.026499  0.027143  0.028917  0.059884  0.088274   \n",
      "\n",
      "term       dick      fag      shut      dumb       wit  \n",
      "proba  0.112871  0.11294  0.115988  0.137491  0.145267  \n",
      "\n",
      "LEAST OFFENSIVE TERMS\n",
      "term    support      beat  terrorist      pick    great      wake     right  \\\n",
      "proba  0.804349  0.809771   0.823323  0.824592  0.82534  0.826038  0.827966   \n",
      "\n",
      "term    outside     bunny       job      mind  christmas       bag     blame  \\\n",
      "proba  0.834079  0.838479  0.842935  0.842981   0.852034  0.878332  0.879693   \n",
      "\n",
      "term      crack  christma     book   version  ultimate       tie  \n",
      "proba  0.880073  0.888287  0.93675  0.947962  0.948976  0.959713  \n"
     ]
    }
   ],
   "source": [
    "vect = pipe_cv_mnb.best_estimator_.steps[0][1]\n",
    "vocab_size = len(vect.vocabulary_)\n",
    "print('Vocabulary size:', vocab_size,'\\n')\n",
    "identity = np.eye(vocab_size)\n",
    "\n",
    "estimator = pipe_cv_mnb.best_estimator_.steps[1][1]\n",
    "words = vect.vocabulary_.keys()\n",
    "probs = pipe_cv_mnb.predict_proba(words)[:,0]\n",
    "\n",
    "words_df = pd.DataFrame(dict(term = list(words), proba = probs))\n",
    "words_df = words_df[['term', 'proba']].set_index('term')\n",
    "\n",
    "print('MOST OFFENSIVE TERMS')\n",
    "print(words_df.sort_values(by = 'proba').head(20).T)\n",
    "print()\n",
    "print('LEAST OFFENSIVE TERMS')\n",
    "print(words_df.sort_values(by = 'proba').tail(20).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_train_pred_mnb = pipe_cv_mnb.predict(X_train)\n",
    "y_test_pred_mnb = pipe_cv_mnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Surprisingly, incorporating additional information about the multiplicity of each term does not improve the model. In fact, Multinomial Naive Bayes performs slightly worse on the training and the test data than its binomial variant. In particular, MNB tends to mis-classify hate speech as offensive and offensive speech as non-offensive more frequently than BNB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.784934497817\n",
      "F1 weighted:  0.769123270586\n",
      "Normalized confusion matrix:\n",
      " [[ 0.49563319  0.01877729  0.00251092]\n",
      " [ 0.05458515  0.24115721  0.03460699]\n",
      " [ 0.02117904  0.08340611  0.0481441 ]]\n",
      "Actual proportions:\n",
      " [ 0.5169214   0.33034934  0.15272926]\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_train, y_train_pred_mnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.774579724911\n",
      "F1 weighted:  0.757285708231\n",
      "Normalized confusion matrix:\n",
      " [[ 0.49388691  0.01986755  0.00305655]\n",
      " [ 0.05399898  0.23790117  0.03846154]\n",
      " [ 0.02317881  0.08685685  0.04279165]]\n",
      "Actual proportions:\n",
      " [ 0.516811    0.33036169  0.15282731]\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test, y_test_pred_mnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Two-class performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.89989811513\n",
      "F1 weighted:  0.899417157232\n",
      "Normalized confusion matrix:\n",
      " [[ 0.49388691  0.0229241 ]\n",
      " [ 0.07717779  0.40601121]]\n",
      "Actual proportions:\n",
      " [ 0.516811  0.483189]\n"
     ]
    }
   ],
   "source": [
    "y_pred_binary_mnb = (y_test_pred_mnb == 1) | (y_test_pred_mnb == 2)\n",
    "evaluate(y_binary, y_pred_binary_mnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### What are these edge cases?\n",
    "Below we example those tweets which BNB correctly classified as offensive where MNB failed to do so as well as offensive tweets correctly classified by both BNB and MNB. The misclassified tweets indeed have fewer offensive words than those correctly classified. In fact, some of the correctly classified tweets have offensive words with multiplicity greater than 1. It would seem that using MNB, by incorprating multiplicity into its model, ends up setting a higher threshold for classifying a tweet as offensive that ultimately harms its performance slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('../data/dataframe_orig', 'rb') as file_in:\n",
    "    df_orig = pkl.load(file_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Mis-classified offensive tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>rating</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>853732524</th>\n",
       "      <td>@kingmark56 seriously, if someone calling you a fag or whatever gets to you that much, it's time to reflect on how you react to things.</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853719706</th>\n",
       "      <td>@mrclungetrumpet ironic bastard that he is.</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853725174</th>\n",
       "      <td>a ngga that keeps a smile on his shorty's face is gon always have her heart b</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853719702</th>\n",
       "      <td>@yung_rell501 i figured it out eventually/ this lil bastard at school</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853728857</th>\n",
       "      <td>@realcurrykid @toastyjaffa you would watch home and away you fag</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853719669</th>\n",
       "      <td>i really want to know carol's maiden name, i don't like that we only know her as that bastard's name</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853732737</th>\n",
       "      <td>live and let live, don't have anything nice to say don't say it, don't ruin someone's day or bring them down just because your a cunt _̫���_̫�_</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853723559</th>\n",
       "      <td>trust it to be my friends that by the time i had a fag and come back down they've been kicked out ffs _�㢉�_�_�㢉�_�_</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853729771</th>\n",
       "      <td>he likes a white trash broad. lmao</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853723035</th>\n",
       "      <td>this game is more butt-ugly than nude pics of janet reno...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                      text  \\\n",
       "id                                                                                                                                                           \n",
       "853732524          @kingmark56 seriously, if someone calling you a fag or whatever gets to you that much, it's time to reflect on how you react to things.   \n",
       "853719706                                                                                                      @mrclungetrumpet ironic bastard that he is.   \n",
       "853725174                                                                    a ngga that keeps a smile on his shorty's face is gon always have her heart b   \n",
       "853719702                                                                            @yung_rell501 i figured it out eventually/ this lil bastard at school   \n",
       "853728857                                                                                 @realcurrykid @toastyjaffa you would watch home and away you fag   \n",
       "853719669                                             i really want to know carol's maiden name, i don't like that we only know her as that bastard's name   \n",
       "853732737  live and let live, don't have anything nice to say don't say it, don't ruin someone's day or bring them down just because your a cunt _̫���_̫�_   \n",
       "853723559                              trust it to be my friends that by the time i had a fag and come back down they've been kicked out ffs _�㢉�_�_�㢉�_�_   \n",
       "853729771                                                                                                               he likes a white trash broad. lmao   \n",
       "853723035                                                                                      this game is more butt-ugly than nude pics of janet reno...   \n",
       "\n",
       "           rating  confidence  \n",
       "id                             \n",
       "853732524       1      1.0000  \n",
       "853719706       1      0.6691  \n",
       "853725174       1      1.0000  \n",
       "853719702       1      0.6718  \n",
       "853728857       1      0.6649  \n",
       "853719669       1      0.6704  \n",
       "853732737       1      0.6788  \n",
       "853723559       1      0.6701  \n",
       "853729771       1      1.0000  \n",
       "853723035       1      0.6582  "
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df_orig.reindex(y_test.index)\n",
    "df_misclf = df_test[(y_test_pred_bnb == 1) & (y_test_pred_mnb == 0) & (y_test == 1)]\n",
    "df_misclf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Correctly classified offensive tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>rating</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>853730295</th>\n",
       "      <td>@_mac33miller_ you know how many people message us asking why ur so obsessed and how annoying you are?? lmfao get the fuck outta here</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853731559</th>\n",
       "      <td>@shawnkta but you think about a rich nigga and his wife. that nigga can buy her anything and she know that. it becomes pointless.</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853730642</th>\n",
       "      <td>@stephwellsb104 @missuniverse @b104 wtf steve haters calling him a nigga i got your back. https://t.co/nq3l0wnc0t</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853727848</th>\n",
       "      <td>all these fucking scratches on me _̫��̍</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853723211</th>\n",
       "      <td>stevie harvey definitely did some coon shit</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853723772</th>\n",
       "      <td>russell wilson a faggot for not fucking ciara yet.... talking all that church bs nigga everybody at the church fucking they just fakin</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853723368</th>\n",
       "      <td>cuffin these bitches you niggas is cops</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853720998</th>\n",
       "      <td>fuck those queers it's all about the vampires _�㢉�__ https://t.co/g3yvllglcz</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853720643</th>\n",
       "      <td>@unroyalreporter i avoid royal pr, didn't watch it. if so, what a fucking hypocrite! unfollowed &amp;gt; @drbrianmay @trussliz @christopherhope</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853732293</th>\n",
       "      <td>@electivirus you're so gay even gay guys go \"wow what a fag\" ;) _̫̍��_�_崁��</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                  text  \\\n",
       "id                                                                                                                                                       \n",
       "853730295        @_mac33miller_ you know how many people message us asking why ur so obsessed and how annoying you are?? lmfao get the fuck outta here   \n",
       "853731559            @shawnkta but you think about a rich nigga and his wife. that nigga can buy her anything and she know that. it becomes pointless.   \n",
       "853730642                            @stephwellsb104 @missuniverse @b104 wtf steve haters calling him a nigga i got your back. https://t.co/nq3l0wnc0t   \n",
       "853727848                                                                                                      all these fucking scratches on me _̫��̍   \n",
       "853723211                                                                                                  stevie harvey definitely did some coon shit   \n",
       "853723772       russell wilson a faggot for not fucking ciara yet.... talking all that church bs nigga everybody at the church fucking they just fakin   \n",
       "853723368                                                                                                      cuffin these bitches you niggas is cops   \n",
       "853720998                                                                 fuck those queers it's all about the vampires _�㢉�__ https://t.co/g3yvllglcz   \n",
       "853720643  @unroyalreporter i avoid royal pr, didn't watch it. if so, what a fucking hypocrite! unfollowed &gt; @drbrianmay @trussliz @christopherhope   \n",
       "853732293                                                                  @electivirus you're so gay even gay guys go \"wow what a fag\" ;) _̫̍��_�_崁��   \n",
       "\n",
       "           rating  confidence  \n",
       "id                             \n",
       "853730295       1      1.0000  \n",
       "853731559       1      1.0000  \n",
       "853730642       1      1.0000  \n",
       "853727848       1      1.0000  \n",
       "853723211       1      0.3385  \n",
       "853723772       1      0.6637  \n",
       "853723368       1      0.6582  \n",
       "853720998       1      0.6615  \n",
       "853720643       1      1.0000  \n",
       "853732293       1      0.6650  "
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df_orig.reindex(y_test.index)\n",
    "df_correct = df_test[(y_test_pred_bnb == 1) & (y_test_pred_mnb == 1) & (y_test == 1)]\n",
    "df_correct.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Multinomial Naive Bayes with TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The TF-IDF model takes the term frequency model a few extra steps. First, whereas previously a term frequency vector consisted of integers representing the multiplicities of terms within a tweet, TF-IDF normalizes these values by dividing each vector element by the number of words in the tweet. Thus, additional information about the length of a tweet is incorporated into the model. The IDF part of the model, inverse document frequency, weights each element of the vector, penalizing terms that appear too frequently in other tweets and boosting rarer terms.\n",
    "\n",
    "Theoretically, such a weighting of vector elements could obviate the need to exclude stop words or set min and max document frequency hyperparameters, but it turns out such parameters are still necessary to tuning an optimal model. In fact, the parameters found through grid search on MNB with TFIDF are the same as those found for a regular count vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:\n",
      " {'vect__max_df': 0.11000000000000001, 'mnb__alpha': 0.0, 'vect__min_df': 0.0060000000000000001}\n",
      "Best accuracy:  0.7675764192139738\n"
     ]
    }
   ],
   "source": [
    "steps = [('vect', TfidfVectorizer()),('mnb', MultinomialNB())]\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "parameters = {'vect__min_df': np.arange(0,0.010, 0.002),\n",
    "              'vect__max_df': np.arange(.10,.145, 0.002),\n",
    "              'mnb__alpha': np.arange(0.0,0.5,0.1)}\n",
    "\n",
    "pipe_cv_tfidf = GridSearchCV(pipe, param_grid = parameters, cv = 3, scoring = 'accuracy')\n",
    "pipe_cv_tfidf.fit(X_train, y_train)\n",
    "\n",
    "print('Best parameters:\\n', pipe_cv_tfidf.best_params_)\n",
    "print('Best accuracy: ', pipe_cv_tfidf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 161 \n",
      "\n",
      "MOST OFFENSIVE TERMS\n",
      "term   bitch  niggas  nigga   faggot    nigger      fuck    fuckin    cuffin  \\\n",
      "proba    0.0     0.0    0.0  0.00418  0.005127  0.012097  0.013469  0.016285   \n",
      "\n",
      "term        ass     queer       hoe      cunt   fucking       gay      shit  \\\n",
      "proba  0.022132  0.024303  0.025979  0.026547  0.026919  0.056546  0.077713   \n",
      "\n",
      "term       shut       fag      dick      dumb       wit  \n",
      "proba  0.090228  0.100843  0.110694  0.115033  0.136061  \n",
      "\n",
      "LEAST OFFENSIVE TERMS\n",
      "term      bunny   support  terrorist      pick      beat   outside     great  \\\n",
      "proba  0.808545  0.809814   0.821787  0.823754  0.823898  0.828287  0.836497   \n",
      "\n",
      "term        job     right      wake  christmas      mind     blame       bag  \\\n",
      "proba  0.841153  0.841729  0.849247   0.853154  0.854688  0.878651  0.881977   \n",
      "\n",
      "term      crack  christma  ultimate      book   version       tie  \n",
      "proba  0.884496  0.894705  0.942934  0.944874  0.956555  0.961924  \n"
     ]
    }
   ],
   "source": [
    "vect = pipe_cv_tfidf.best_estimator_.steps[0][1]\n",
    "vocab_size = len(vect.vocabulary_)\n",
    "print('Vocabulary size:', vocab_size,'\\n')\n",
    "identity = np.eye(vocab_size)\n",
    "\n",
    "estimator = pipe_cv_tfidf.best_estimator_.steps[1][1]\n",
    "words = vect.vocabulary_.keys()\n",
    "probs = pipe_cv_tfidf.predict_proba(words)[:,0]\n",
    "\n",
    "words_df = pd.DataFrame(dict(term = list(words), proba = probs))\n",
    "words_df = words_df[['term', 'proba']].set_index('term')\n",
    "\n",
    "print('MOST OFFENSIVE TERMS')\n",
    "print(words_df.sort_values(by = 'proba').head(20).T)\n",
    "print()\n",
    "print('LEAST OFFENSIVE TERMS')\n",
    "print(words_df.sort_values(by = 'proba').tail(20).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_train_pred_tfidf = pipe_cv_tfidf.predict(X_train)\n",
    "y_test_pred_tfidf = pipe_cv_tfidf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In yet another surprise, the performance of MNB using the more sophisticated TFIDF model is worse still than the simplier term frequency model and the even simplier BNB model. It may be that tweets are fundamentally too short and the dataset too small for the application of more complex models. Additionally, the nature of the classification problem, whether or not a tweet is offensive, hinges only on the presence of a small set of terms when modelling each tweet as a bag of words. As such, incorporating multiplicity and IDF contributes too little useful information to offset the cost of modeling with greater complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.772707423581\n",
      "F1 weighted:  0.741128916772\n",
      "Normalized confusion matrix:\n",
      " [[ 0.49912664  0.01670306  0.0010917 ]\n",
      " [ 0.06593886  0.24781659  0.01659389]\n",
      " [ 0.02620087  0.10076419  0.02576419]]\n",
      "Actual proportions:\n",
      " [ 0.5169214   0.33034934  0.15272926]\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_train, y_train_pred_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.770249617932\n",
      "F1 weighted:  0.738183633344\n",
      "Normalized confusion matrix:\n",
      " [[ 0.49872644  0.01706572  0.00101885]\n",
      " [ 0.06418747  0.24732552  0.0188487 ]\n",
      " [ 0.02496179  0.10366786  0.02419766]]\n",
      "Actual proportions:\n",
      " [ 0.516811    0.33036169  0.15282731]\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test, y_test_pred_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Two-class performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.892766174223\n",
      "F1 weighted:  0.891959565203\n",
      "Normalized confusion matrix:\n",
      " [[ 0.49872644  0.01808456]\n",
      " [ 0.08914926  0.39403974]]\n",
      "Actual proportions:\n",
      " [ 0.516811  0.483189]\n"
     ]
    }
   ],
   "source": [
    "y_pred_binary_tfidf = (y_test_pred_tfidf == 1) | (y_test_pred_tfidf == 2)\n",
    "evaluate(y_binary, y_pred_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Tweet rater\n",
    "Finally, below is a simple function that uses the BNB model to rate new user-generated tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def replace_user(tweet):\n",
    "    return re.sub(r'(@\\w+\\s*)', r'<user> ', tweet)\n",
    "\n",
    "def replace_url(tweet):\n",
    "    return re.sub(r'(https?://\\S*)', r'<url> )', tweet)\n",
    "\n",
    "def erase_special(tweet):\n",
    "    regex = r'#|&|\\(|\\)|\\\"|\\.|\\?|!|,|;|:|(�\\S*\\d*)|(_*UNDEF)|\\\\n|\\s\\'|\\'\\s|-|/|$|%|\\n|{|}|[|]|~'\n",
    "    return re.sub(regex, ' ', tweet)\n",
    "\n",
    "def erase_numbers(tweet):\n",
    "    regex = r'(128\\d{3})|(82\\d{2})'\n",
    "    return re.sub(regex, ' ', tweet)\n",
    "\n",
    "def normalize(tweet):\n",
    "    x = tweet.split()\n",
    "    y = ''\n",
    "    for token in x:\n",
    "        y = ' '.join([y,token.lower()])\n",
    "    return y[1:]\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    tweet = replace_user(tweet)\n",
    "    tweet = replace_url(tweet)\n",
    "    tweet = erase_numbers(tweet)\n",
    "    tweet = erase_special(tweet)\n",
    "    return tweet\n",
    "\n",
    "def lemmatize(tweet):\n",
    "    x = str()\n",
    "    for token in nlp(tweet):\n",
    "        if token.text in ['user','url','<','>']:\n",
    "            continue\n",
    "        else:\n",
    "            x = ' '.join([x, token.lemma_])\n",
    "    return x[1:]\n",
    "\n",
    "def remove_stop(tweet):\n",
    "    x = str()\n",
    "    for token in nlp(tweet):\n",
    "        if token.text == '-PRON-':\n",
    "            continue\n",
    "        if token.is_stop:\n",
    "            continue\n",
    "        else:\n",
    "            x = ' '.join([x, token.text])\n",
    "    return x[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def tweet_rater(tweet, clf):\n",
    "    tweet = clean_tweet(tweet)\n",
    "    tweet = lemmatize(tweet)\n",
    "    tweet = remove_stop(tweet)\n",
    "    \n",
    "    print(tweet)\n",
    "    rating = clf.predict([tweet])\n",
    "    probability = clf.predict_proba([tweet])\n",
    "    print(probability)\n",
    "    if rating == 0:\n",
    "        print('I\\'m {:2.4}% sure that\\'s not offensive.'.format(probability[0][0]*100))\n",
    "    elif rating == 1:\n",
    "        print('I\\'m {:2.4}% sure that\\'s offensive.'.format(probability[0][1]*100))\n",
    "    else:\n",
    "        print('I\\'m {:2.4}% sure that\\'s hate speech.'.format(probability[0][2]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hate\n",
      "[[ 0.33362046  0.36367828  0.30270126]]\n",
      "I'm 36.37% sure that's offensive.\n"
     ]
    }
   ],
   "source": [
    "tweet_rater('i hate you all', pipe_cv_bnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
